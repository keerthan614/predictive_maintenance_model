{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Fundamentals Worksheet\n",
    "\n",
    "Welcome to this hands-on exploration of machine learning fundamentals! In this notebook, you'll learn about:\n",
    "1. The ML process overview\n",
    "2. K-Nearest Neighbors algorithm\n",
    "3. Measuring and understanding error\n",
    "\n",
    "Let's start by importing the libraries we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Set up matplotlib for better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries imported successfully! Let's begin! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Overview of Machine Learning Process (15 minutes)\n",
    "\n",
    "Machine learning follows a systematic process. Let's understand it step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: The ML Pipeline\n",
    "\n",
    "Below is a scrambled list of steps in the machine learning process. **Write the correct order (1-7) in the comments:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the correct order (1-7) for each step:\n",
    "\n",
    "steps = {\n",
    "    \"Deploy and monitor the model\": None,          # Order: ?\n",
    "    \"Collect and prepare data\": None,              # Order: ?\n",
    "    \"Define the problem and goals\": None,          # Order: ?\n",
    "    \"Evaluate model performance\": None,            # Order: ?\n",
    "    \"Choose and train a model\": None,              # Order: ?\n",
    "    \"Split data into train/validation/test\": None, # Order: ?\n",
    "    \"Feature engineering and selection\": None      # Order: ?\n",
    "}\n",
    "\n",
    "# TODO: Fill in the correct numbers above\n",
    "# Hint: Think about what you need to do first, second, etc.\n",
    "\n",
    "print(\"Once you fill in the orders, run this cell to see your answers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Problem Classification\n",
    "\n",
    "Let's practice identifying different types of ML problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match each problem to its ML task type\n",
    "# Types: 'classification', 'regression', 'clustering', 'recommendation'\n",
    "\n",
    "problems = {\n",
    "    \"Predicting house prices\": None,               # TODO: Fill this in\n",
    "    \"Detecting spam emails\": None,                 # TODO: Fill this in\n",
    "    \"Recommending movies to users\": None,          # TODO: Fill this in\n",
    "    \"Grouping customers by behavior\": None,        # TODO: Fill this in\n",
    "    \"Predicting stock prices\": None,               # TODO: Fill this in\n",
    "    \"Diagnosing medical conditions\": None          # TODO: Fill this in\n",
    "}\n",
    "\n",
    "# Check your work:\n",
    "for problem, task_type in problems.items():\n",
    "    print(f\"{problem}: {task_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Quick Warm-up - Data Visualization\n",
    "\n",
    "Let's start with a simple exercise to get comfortable with plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some simple 2D data\n",
    "cats = [(1, 2), (2, 1), (1, 3), (0, 2)]\n",
    "dogs = [(4, 4), (5, 3), (4, 5), (6, 4)]\n",
    "\n",
    "# TODO: Plot the cats and dogs data\n",
    "# Hint: Use plt.scatter() with different colors for cats and dogs\n",
    "\n",
    "# Extract x and y coordinates for cats\n",
    "cat_x = [point[0] for point in cats]  # TODO: Complete this line\n",
    "cat_y = # TODO: Complete this line\n",
    "\n",
    "# Extract x and y coordinates for dogs  \n",
    "dog_x = # TODO: Complete this line\n",
    "dog_y = # TODO: Complete this line\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# TODO: Add scatter plots for cats (use 'blue' and 'o' marker)\n",
    "# TODO: Add scatter plots for dogs (use 'red' and 's' marker)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Cats vs Dogs Dataset')\n",
    "plt.legend(['Cats', 'Dogs'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"We have {len(cats)} cats and {len(dogs)} dogs in our dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: K-Nearest Neighbors Algorithm\n",
    "\n",
    "Now let's dive into K-NN! We'll start with understanding it conceptually, then implement it step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Understanding Distance\n",
    "\n",
    "The foundation of K-NN is measuring distance between points. Let's start with a simple exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate distance manually first\n",
    "point_a = (1, 2)\n",
    "point_b = (4, 6)\n",
    "\n",
    "# TODO: Calculate Euclidean distance between point_a and point_b\n",
    "# Formula: sqrt((x1-x2)¬≤ + (y1-y2)¬≤)\n",
    "\n",
    "x_diff = # TODO: Calculate x difference\n",
    "y_diff = # TODO: Calculate y difference\n",
    "distance = # TODO: Calculate the final distance using math.sqrt()\n",
    "\n",
    "print(f\"Point A: {point_a}\")\n",
    "print(f\"Point B: {point_b}\")\n",
    "print(f\"Distance: {distance:.2f}\")\n",
    "\n",
    "# Let's visualize this\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot([point_a[0], point_b[0]], [point_a[1], point_b[1]], 'r--', linewidth=2, label=f'Distance: {distance:.2f}')\n",
    "plt.scatter([point_a[0]], [point_a[1]], color='blue', s=100, label='Point A')\n",
    "plt.scatter([point_b[0]], [point_b[1]], color='red', s=100, label='Point B')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.title('Distance Between Two Points')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Implement Distance Function\n",
    "\n",
    "Now let's code our distance function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(point1, point2):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance between two points.\n",
    "    \n",
    "    Args:\n",
    "        point1: tuple of (x, y)\n",
    "        point2: tuple of (x, y)\n",
    "    \n",
    "    Returns:\n",
    "        float: Euclidean distance\n",
    "    \"\"\"\n",
    "    # TODO: Implement the distance formula\n",
    "    # Hint: Use math.sqrt() and remember the formula from above\n",
    "    \n",
    "    return # TODO: Return the calculated distance\n",
    "\n",
    "# Test your function\n",
    "test_cases = [\n",
    "    ((0, 0), (3, 4)),  # Should be 5.0\n",
    "    ((1, 1), (1, 1)),  # Should be 0.0\n",
    "    ((0, 0), (1, 1))   # Should be ~1.41\n",
    "]\n",
    "\n",
    "print(\"Testing your distance function:\")\n",
    "for i, (p1, p2) in enumerate(test_cases):\n",
    "    result = euclidean_distance(p1, p2)\n",
    "    print(f\"Test {i+1}: Distance from {p1} to {p2} = {result:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Manual K-NN Classification\n",
    "\n",
    "Before we code the full algorithm, let's do a manual classification to understand the concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our training data: (x, y, label)\n",
    "training_data = [\n",
    "    (1, 1, 'cat'), (2, 1, 'cat'), (1, 2, 'cat'),\n",
    "    (5, 5, 'dog'), (6, 5, 'dog'), (5, 6, 'dog')\n",
    "]\n",
    "\n",
    "# Point we want to classify\n",
    "test_point = (3, 3)\n",
    "\n",
    "print(\"Manual K-NN Classification\")\n",
    "print(f\"Test point: {test_point}\")\n",
    "print(\"\\nCalculating distances to all training points:\")\n",
    "\n",
    "distances = []\n",
    "for x, y, label in training_data:\n",
    "    dist = euclidean_distance(test_point, (x, y))\n",
    "    distances.append((x, y, label, dist))\n",
    "    print(f\"Distance to ({x}, {y}, '{label}'): {dist:.2f}\")\n",
    "\n",
    "# TODO: Sort distances and find the 3 nearest neighbors\n",
    "distances.sort(key=lambda x: x[3])  # Sort by distance (4th element)\n",
    "k = 3\n",
    "nearest_neighbors = distances[:k]\n",
    "\n",
    "print(f\"\\n{k} Nearest Neighbors:\")\n",
    "for x, y, label, dist in nearest_neighbors:\n",
    "    print(f\"  ({x}, {y}, '{label}') - Distance: {dist:.2f}\")\n",
    "\n",
    "# TODO: Count votes and make prediction\n",
    "votes = {}\n",
    "for _, _, label, _ in nearest_neighbors:\n",
    "    # TODO: Count the votes for each label\n",
    "    pass\n",
    "\n",
    "# TODO: Find the label with most votes\n",
    "predicted_class = # TODO: Complete this\n",
    "\n",
    "print(f\"\\nVotes: {votes}\")\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4: Visualizing K-NN\n",
    "\n",
    "Let's visualize what's happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the K-NN process\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot training data\n",
    "for x, y, label in training_data:\n",
    "    color = 'blue' if label == 'cat' else 'red'\n",
    "    marker = 'o' if label == 'cat' else 's'\n",
    "    plt.scatter(x, y, color=color, marker=marker, s=100, alpha=0.7)\n",
    "\n",
    "# Plot test point\n",
    "plt.scatter(test_point[0], test_point[1], color='green', marker='*', s=200, label='Test Point')\n",
    "\n",
    "# TODO: Draw lines to the k nearest neighbors\n",
    "for x, y, label, dist in nearest_neighbors:\n",
    "    # TODO: Draw a line from test_point to this neighbor\n",
    "    # Hint: Use plt.plot([x1, x2], [y1, y2], '--', alpha=0.5)\n",
    "    pass\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title(f'K-NN Classification (K={k})')\n",
    "plt.legend(['Cats', 'Dogs', 'Test Point'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The test point {test_point} is classified as: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5: Complete K-NN Implementation\n",
    "\n",
    "Now let's implement the complete K-NN algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_nearest(test_point, training_data, k):\n",
    "    \"\"\"\n",
    "    Find the k nearest neighbors to a test point.\n",
    "    \n",
    "    Args:\n",
    "        test_point: tuple of (x, y)\n",
    "        training_data: list of tuples [(x, y, label), ...]\n",
    "        k: number of neighbors to find\n",
    "    \n",
    "    Returns:\n",
    "        list: k nearest neighbors with distances\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    \n",
    "    # TODO: Calculate distance to each training point\n",
    "    for x, y, label in training_data:\n",
    "        # TODO: Use your euclidean_distance function\n",
    "        dist = \n",
    "        distances.append((x, y, label, dist))\n",
    "    \n",
    "    # TODO: Sort by distance and return k nearest\n",
    "    distances.sort(key=lambda x: x[3])\n",
    "    return distances[:k]\n",
    "\n",
    "def predict_class(neighbors):\n",
    "    \"\"\"\n",
    "    Predict class based on majority vote.\n",
    "    \n",
    "    Args:\n",
    "        neighbors: list of tuples [(x, y, label, distance), ...]\n",
    "    \n",
    "    Returns:\n",
    "        string: predicted class label\n",
    "    \"\"\"\n",
    "    # TODO: Count votes for each class\n",
    "    votes = {}\n",
    "    for _, _, label, _ in neighbors:\n",
    "        # TODO: Update vote count\n",
    "        pass\n",
    "    \n",
    "    # TODO: Return the class with most votes\n",
    "    return max(votes, key=votes.get)\n",
    "\n",
    "def knn_classifier(test_point, training_data, k):\n",
    "    \"\"\"\n",
    "    Complete K-NN classifier.\n",
    "    \"\"\"\n",
    "    neighbors = find_k_nearest(test_point, training_data, k)\n",
    "    prediction = predict_class(neighbors)\n",
    "    return prediction, neighbors\n",
    "\n",
    "# Test your implementation\n",
    "test_points = [(2, 2), (4, 4), (3, 3)]\n",
    "\n",
    "print(\"Testing K-NN Implementation:\")\n",
    "for point in test_points:\n",
    "    pred, neighbors = knn_classifier(point, training_data, k=3)\n",
    "    print(f\"\\nPoint {point}: Predicted as '{pred}'\")\n",
    "    print(f\"  Based on neighbors: {[(x, y, label) for x, y, label, _ in neighbors]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.6: Experimenting with Different K Values\n",
    "\n",
    "Let's see how different K values affect our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different K values\n",
    "test_point = (3, 3)\n",
    "k_values = [1, 3, 5]\n",
    "\n",
    "print(f\"Predictions for point {test_point} with different K values:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for k in k_values:\n",
    "    if k <= len(training_data):  # Make sure K is not larger than dataset\n",
    "        pred, neighbors = knn_classifier(test_point, training_data, k)\n",
    "        \n",
    "        # Count votes\n",
    "        votes = Counter([label for _, _, label, _ in neighbors])\n",
    "        \n",
    "        print(f\"K={k}: Prediction = '{pred}'\")\n",
    "        print(f\"     Votes = {dict(votes)}\")\n",
    "        print()\n",
    "\n",
    "# TODO: Answer these questions:\n",
    "questions = \"\"\"\n",
    "Think about these questions:\n",
    "1. Which K value seems most reliable for this dataset?\n",
    "2. What happens when K=1? Is it stable?\n",
    "3. What would happen if K was equal to the size of our dataset?\n",
    "\n",
    "Write your answers here:\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "\"\"\"\n",
    "\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Measuring and Understanding Error (12 minutes)\n",
    "\n",
    "Now let's learn how to measure how good our K-NN classifier is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Simple Accuracy Calculation\n",
    "\n",
    "Let's start with a basic accuracy calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculate classification accuracy.\n",
    "    \n",
    "    Args:\n",
    "        true_labels: list of actual labels\n",
    "        predicted_labels: list of predicted labels\n",
    "    \n",
    "    Returns:\n",
    "        float: accuracy as a percentage\n",
    "    \"\"\"\n",
    "    # TODO: Calculate how many predictions are correct\n",
    "    correct = 0\n",
    "    total = len(true_labels)\n",
    "    \n",
    "    for i in range(total):\n",
    "        # TODO: Check if prediction matches true label\n",
    "        if :\n",
    "            correct += 1\n",
    "    \n",
    "    # TODO: Return accuracy as percentage\n",
    "    return (correct / total) * 100\n",
    "\n",
    "# Test your function\n",
    "true = ['cat', 'dog', 'cat', 'dog', 'cat']\n",
    "pred = ['cat', 'dog', 'dog', 'dog', 'cat']\n",
    "\n",
    "accuracy = calculate_accuracy(true, pred)\n",
    "print(f\"True labels:      {true}\")\n",
    "print(f\"Predicted labels: {pred}\")\n",
    "print(f\"Accuracy: {accuracy}%\")\n",
    "\n",
    "# TODO: Can you identify which predictions were wrong?\n",
    "print(\"\\nCorrect predictions:\")\n",
    "for i, (t, p) in enumerate(zip(true, pred)):\n",
    "    status = \"‚úì\" if t == p else \"‚úó\"\n",
    "    print(f\"  Position {i}: True={t}, Predicted={p} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Creating a Test Dataset\n",
    "\n",
    "Let's create a larger dataset to test our K-NN properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger, more realistic dataset\n",
    "np.random.seed(42)  # For reproducible results\n",
    "\n",
    "# Generate cat data (clustered around (2, 2))\n",
    "n_cats = 15\n",
    "cat_data = []\n",
    "for i in range(n_cats):\n",
    "    x = np.random.normal(2, 1)  # Mean=2, std=1\n",
    "    y = np.random.normal(2, 1)\n",
    "    cat_data.append((x, y, 'cat'))\n",
    "\n",
    "# Generate dog data (clustered around (6, 6))\n",
    "n_dogs = 15\n",
    "dog_data = []\n",
    "for i in range(n_dogs):\n",
    "    x = np.random.normal(6, 1)  # Mean=6, std=1\n",
    "    y = np.random.normal(6, 1)\n",
    "    dog_data.append((x, y, 'dog'))\n",
    "\n",
    "# Combine all training data\n",
    "full_training_data = cat_data + dog_data\n",
    "\n",
    "print(f\"Created dataset with {len(cat_data)} cats and {len(dog_data)} dogs\")\n",
    "\n",
    "# TODO: Visualize the new dataset\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot cats\n",
    "cat_x = [x for x, y, label in full_training_data if label == 'cat']\n",
    "cat_y = [y for x, y, label in full_training_data if label == 'cat']\n",
    "\n",
    "# Plot dogs  \n",
    "dog_x = [x for x, y, label in full_training_data if label == 'dog']\n",
    "dog_y = [y for x, y, label in full_training_data if label == 'dog']\n",
    "\n",
    "# TODO: Create scatter plots for cats and dogs\n",
    "plt.scatter(cat_x, cat_y, color='blue', marker='o', alpha=0.7, label='Cats')\n",
    "plt.scatter(dog_x, dog_y, color='red', marker='s', alpha=0.7, label='Dogs')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Larger Cat vs Dog Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Testing Different K Values\n",
    "\n",
    "Now let's systematically test different K values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "test_data = [\n",
    "    (1.5, 1.8, 'cat'), (2.2, 2.5, 'cat'), (1.8, 1.2, 'cat'),\n",
    "    (2.8, 2.1, 'cat'), (1.9, 2.8, 'cat'),\n",
    "    (5.8, 6.2, 'dog'), (6.1, 5.9, 'dog'), (5.5, 6.5, 'dog'),\n",
    "    (6.3, 5.7, 'dog'), (5.9, 6.1, 'dog')\n",
    "]\n",
    "\n",
    "# Test different K values\n",
    "k_values = [1, 3, 5, 7, 9]\n",
    "accuracies = []\n",
    "\n",
    "print(\"Testing different K values:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for k in k_values:\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    # Make predictions for all test points\n",
    "    for x, y, true_label in test_data:\n",
    "        pred, _ = knn_classifier((x, y), full_training_data, k)\n",
    "        predictions.append(pred)\n",
    "        true_labels.append(true_label)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = calculate_accuracy(true_labels, predictions)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"K={k}: Accuracy = {accuracy:.1f}%\")\n",
    "\n",
    "# TODO: Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "# TODO: Create a line plot of K values vs accuracies\n",
    "plt.plot(k_values, accuracies, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('K-NN Accuracy vs K Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "\n",
    "# Add accuracy values on the plot\n",
    "for k, acc in zip(k_values, accuracies):\n",
    "    plt.annotate(f'{acc:.1f}%', (k, acc), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "best_k = k_values[accuracies.index(max(accuracies))]\n",
    "print(f\"\\nBest K value: {best_k} with accuracy: {max(accuracies):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4: Understanding Overfitting and Underfitting\n",
    "\n",
    "Let's explore what happens with extreme K values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test extreme K values\n",
    "extreme_k_values = [1, len(full_training_data)//2, len(full_training_data)-1]\n",
    "\n",
    "print(\"Understanding Extreme K Values:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for k in extreme_k_values:\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for x, y, true_label in test_data:\n",
    "        pred, neighbors = knn_classifier((x, y), full_training_data, k)\n",
    "        predictions.append(pred)\n",
    "        true_labels.append(true_label)\n",
    "    \n",
    "    accuracy = calculate_accuracy(true_labels, predictions)\n",
    "    \n",
    "    print(f\"\\nK={k}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.1f}%\")\n",
    "    \n",
    "    if k == 1:\n",
    "        print(\"  ‚Üí This is very sensitive to noise (HIGH VARIANCE)\")\n",
    "        print(\"  ‚Üí May overfit to training data\")\n",
    "    elif k == len(full_training_data)-1:\n",
    "        print(\"  ‚Üí This always predicts the majority class (HIGH BIAS)\")\n",
    "        print(\"  ‚Üí May underfit the data\")\n",
    "    else:\n",
    "        print(\"  ‚Üí This balances bias and variance\")\n",
    "\n",
    "# TODO: Fill in your observations\n",
    "observations = \"\"\"\n",
    "Your observations:\n",
    "1. What happened when K=1? Why might this be problematic?\n",
    "   Answer: \n",
    "\n",
    "2. What happened when K was very large? What does this tell us?\n",
    "   Answer: \n",
    "\n",
    "3. How do you think you should choose K in practice?\n",
    "   Answer: \n",
    "\"\"\"\n",
    "\n",
    "print(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Integration and Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Challenge\n",
    "\n",
    "Can you improve the K-NN classifier? Try these modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_knn_classifier(test_point, training_data, k):\n",
    "    \"\"\"\n",
    "    K-NN with distance weighting - closer neighbors have more influence.\n",
    "    \"\"\"\n",
    "    neighbors = find_k_nearest(test_point, training_data, k)\n",
    "    \n",
    "    # TODO: Instead of simple voting, weight votes by inverse distance\n",
    "    # Hint: weight = 1 / (distance + 0.0001)  # Add small value to avoid division by 0\n",
    "    \n",
    "    weighted_votes = {}\n",
    "    \n",
    "    for x, y, label, dist in neighbors:\n",
    "        # TODO: Calculate weight and add to weighted_votes\n",
    "        weight = 1 / (dist + 0.0001)\n",
    "        if label not in weighted_votes:\n",
    "            weighted_votes[label] = 0\n",
    "        weighted_votes[label] += weight\n",
    "    \n",
    "    # Return the class with highest weighted vote\n",
    "    return max(weighted_votes, key=weighted_votes.get)\n",
    "\n",
    "# Test both classifiers\n",
    "print(\"Comparing Regular K-NN vs Weighted K-NN:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "regular_predictions = []\n",
    "weighted_predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for x, y, true_label in test_data:\n",
    "    regular_pred, _ = knn_classifier((x, y), full_training_data, 5)\n",
    "    weighted_pred = weighted_knn_classifier((x, y), full_training_data, 5)\n",
    "    \n",
    "    regular_predictions.append(regular_pred)\n",
    "    weighted_predictions.append(weighted_pred)\n",
    "    true_labels.append(true_label)\n",
    "\n",
    "regular_acc = calculate_accuracy(true_labels, regular_predictions)\n",
    "weighted_acc = calculate_accuracy(true_labels, weighted_predictions)\n",
    "\n",
    "print(f\"Regular K-NN accuracy:  {regular_acc:.1f}%\")\n",
    "print(f\"Weighted K-NN accuracy: {weighted_acc:.1f}%\")\n",
    "\n",
    "if weighted_acc > regular_acc:\n",
    "    print(\"üéâ Great! Weighted K-NN performed better!\")\n",
    "elif weighted_acc == regular_acc:\n",
    "    print(\"Both methods performed equally well!\")\n",
    "else:\n",
    "    print(\"Interesting! Regular K-NN was better this time.\")\n",
    "\n",
    "print(\"\\nüèÜ Congratulations! You've completed the K-NN workshop!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "**What we learned today:**\n",
    "\n",
    "1. **Machine Learning Process**: The systematic steps from problem definition to deployment\n",
    "2. **K-Nearest Neighbors**: A simple but powerful algorithm based on distance and voting\n",
    "3. **Error Measurement**: How to evaluate classifier performance and choose good parameters\n",
    "\n",
    "**Key Insights:**\n",
    "- K-NN is intuitive: \"Tell me who your neighbors are, and I'll tell you who you are\"\n",
    "- Choosing K is about balancing bias (too high K) and variance (too low K)\n",
    "- Distance matters: the way we measure similarity affects everything\n",
    "- Simple algorithms can work surprisingly well!\n",
    "\n",
    "**Next Steps:**\n",
    "- Try K-NN on real datasets\n",
    "- Experiment with different distance metrics\n",
    "- Learn about other algorithms like Decision Trees and Neural Networks\n",
    "\n",
    "Great job completing this hands-on machine learning workshop! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
